{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Collection\n",
    
    "(Don't need to run these cells if files already downloaded from the first code file)\n",
    "\n",
    "a) Scrapes the website and downloads all of Obama's speeches as PDF's [378 Speeches Downloaded]\n",
    "\n",
    "b) Converts PDF's into text files (.txt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.americanrhetoric.com/barackobamaspeeches.htm\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}\n",
    "\n",
    "folder_location = r'webscraping_final'\n",
    "if not os.path.exists(folder_location):\n",
    "   os.mkdir(folder_location)\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "print(response.text)\n",
    "for link in soup.select(\"a[href$='.pdf']\"):\n",
    "   filename = os.path.join(folder_location,link['href'].split('/')[-1])\n",
    "   with open(filename, 'wb') as f:\n",
    "      f.write(requests.get(urljoin(url,link['href']), headers=headers).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tika import parser\n",
    "directory = r'webscraping_final'\n",
    "folder_location = r'textdata_final'\n",
    "if not os.path.exists(folder_location):\n",
    "\tos.mkdir(folder_location)\n",
    "for filename in os.listdir(directory):\n",
    "    print (filename)\n",
    "    input_file= directory + \"/\" + filename\n",
    "    file_data = parser.from_file(input_file)\n",
    "    text = file_data['content']\n",
    "    output_filepath= folder_location + \"/\" + filename + '.txt'\n",
    "    f = open(output_filepath, \"a\")\n",
    "    f.write(text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    " a) Text Standardization: Expanding Contractions \n",
    " \n",
    " b) Tokenization\n",
    " \n",
    " c) Lemmatization\n",
    " \n",
    " d) TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.05388055 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.01869618 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.01229029 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.00581359 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation: Cleans each speech by removing stop words & lemmatizing words to make sure they are of the same base\n",
    "# Then, vectorized each speech and used inverse document frequency to get the relative importance (weight) of each word\n",
    "# Lastly, used sklearn's train-test-split and Gaussian Naive Bayes Classifier\n",
    "\n",
    "contractions = {\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I would\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"so've\": \"so have\",\n",
    "\"that's\": \"that is\",\n",
    "\"that'll\": \"that will\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"y'all\": \"you all\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\"Biden's\": \"Biden is\",\n",
    "} #Dictionary with contractions: For each speech, I removed the contractions and made it to 2 separate words\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import twitter_samples \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk import FreqDist\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "text_document_list = []\n",
    "sentiment_list = [] #polarity associated with each speech from textblob\n",
    "date_speeches = []  #Adds the date for every speech\n",
    "speeches = glob.glob(\"*.txt\")\n",
    "path_to_speeches = \"/Users/dhruv_batra/Desktop/SentimentAnalysis_Final/\"\n",
    "\n",
    "#Goes through each speech and adds the sentiment analysis score through TextBlob into the sentiment list\n",
    "#Used TextBlob as the true y values for my Gaussian Naive Bayes Model\n",
    "for file in glob.glob(path_to_speeches + \"*.txt\"):\n",
    "    speech = open(file,\"r\").read()\n",
    "    date_speeches.append(Path(file).stem[-10:])\n",
    "    if(TextBlob(speech).sentiment.polarity*1000 <= -200): #multiplied sentiment by 1000 for easier readability\n",
    "        sentiment_list.append(\"Very Negative\")\n",
    "    elif(-200 < TextBlob(speech).sentiment.polarity*1000 < 0):\n",
    "        sentiment_list.append(\"Negative\")\n",
    "    elif(TextBlob(speech).sentiment.polarity*1000 == 0):\n",
    "        sentiment_list.append(\"Neutral\")\n",
    "    elif(0 < TextBlob(speech).sentiment.polarity*1000 <= 250):\n",
    "        sentiment_list.append(\"Positive\")\n",
    "    elif(TextBlob(speech).sentiment.polarity*1000 > 250):\n",
    "        sentiment_list.append(\"Very Positive\")\n",
    "\n",
    "    for word in speech.split():\n",
    "        if word.lower() in contractions: #replaces all contractions as 2 words\n",
    "            speech = speech.replace(word, contractions[word.lower()])\n",
    "\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\") #Only contains alphanumeric characters (no punctuation)\n",
    "    speech_tokens = tokenizer.tokenize(speech) #Tokenization\n",
    "\n",
    "#Changes form of words:Lemmatizing\n",
    "    def lemmatizer(speech_tokens):\n",
    "        new_speech = []\n",
    "        for word, tag in pos_tag(speech_tokens):\n",
    "            if (tag[0:2] == \"NN\"):\n",
    "                pos = \"n\"\n",
    "            elif (tag[0:2] == \"VB\"):\n",
    "                pos = \"v\"\n",
    "            else:\n",
    "                pos = \"a\"\n",
    "            new_speech.append(WordNetLemmatizer().lemmatize(word,pos))\n",
    "        return new_speech\n",
    "\n",
    "#Removes unneccessary stuff/noise from Data: removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    speech_new = lemmatizer(speech_tokens)\n",
    "    speech_minus_stopwords = []\n",
    "\n",
    "    for word in speech_new:\n",
    "        if word.lower() not in stop_words:\n",
    "            speech_minus_stopwords.append(word)\n",
    "\n",
    "    filtered_speech = speech_minus_stopwords\n",
    "    filtered_speech_lower = []\n",
    "    for word in filtered_speech:\n",
    "        filtered_speech_lower.append(word.lower()) #Added all of the words that were not stop words into a list\n",
    "    \n",
    "    speech_str = \" \".join(filtered_speech_lower) #joined the words of each speech in the list to a string\n",
    "    text_document_list.append(speech_str)\n",
    "\n",
    "#Vectorized every speech in the text_document_list\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text_document_list)\n",
    "\n",
    "\n",
    "counts = vectorizer.transform(text_document_list)\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_tf = count_vect.fit_transform(text_document_list)\n",
    "\n",
    "#Inverse Document Frequency to account for the commonality of each word in the speech\n",
    "vectorize = TfidfTransformer()\n",
    "vectorize.fit(counts)\n",
    "freq = vectorize.transform(counts)\n",
    "\n",
    "X_tfidf = vectorize.fit_transform(X_tf)\n",
    "\n",
    "X_final = X_tfidf.toarray()\n",
    "print(X_final) #2D Array of all speeches where each row vector contains the vectorized form for each speech\n",
    "\n",
    "y = np.array(sentiment_list) #list of all TextBlob sentiment scores associated with each speech\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "a) Train-Test-Split\n",
    "\n",
    "b) Algorithm: SciKit Learn's Gaussian Naive Bayes\n",
    "\n",
    "c) Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: \n",
      "[[0.         0.01312801 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.0215482  0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.00499305 0.         ... 0.         0.         0.        ]]\n",
      "(264, 18362)\n",
      "X_test: \n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.01406815 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "(114, 18362)\n",
      "y_train: \n",
      "['Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Very Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Very Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Very Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Very Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Negative' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Very Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Very Positive' 'Positive' 'Positive']\n",
      "(264,)\n",
      "y_test: \n",
      "['Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Very Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive']\n",
      "(114,)\n"
     ]
    }
   ],
   "source": [
    "# 70% Training and 30% Testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.30)\n",
    "print(\"X_train: \")\n",
    "print(X_train)\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"X_test: \")\n",
    "print(X_test)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(\"y_train: \")\n",
    "print(y_train)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"y_test: \")\n",
    "print(y_test)\n",
    "print(y_test.shape)\n",
    "\n",
    "#Gaussian Naive Bayes Model\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive'\n",
      " 'Positive' 'Positive' 'Positive' 'Positive' 'Positive' 'Positive']\n"
     ]
    }
   ],
   "source": [
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 114 points : 1\n",
      "Percentage Correct w/ Gaussian Naive Bayes Classifier: 99.12280701754386%\n"
     ]
    }
   ],
   "source": [
    "#Determines the number of points that have been labeled incorrectly based on y_test\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "       % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "print(\"Percentage Correct w/ Gaussian Naive Bayes Classifier: \" + str((1-((y_test != y_pred).sum())/X_test.shape[0])*100) + \"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
