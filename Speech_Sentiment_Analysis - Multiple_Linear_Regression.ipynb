{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Collection\n",
    "a) Scrapes the website and downloads all of Obama's speeches as PDF's [378 Speeches Downloaded]\n",
    "\n",
    "b) Converts PDF's into text files (.txt) \n",
    "\n",
    "c) Changes the name of each file into the date the speech was given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.americanrhetoric.com/barackobamaspeeches.htm\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}\n",
    "\n",
    "folder_location = r'webscraping_final'\n",
    "if not os.path.exists(folder_location):\n",
    "   os.mkdir(folder_location)\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "print(response.text)\n",
    "for link in soup.select(\"a[href$='.pdf']\"):\n",
    "   filename = os.path.join(folder_location,link['href'].split('/')[-1])\n",
    "   with open(filename, 'wb') as f:\n",
    "      f.write(requests.get(urljoin(url,link['href']), headers=headers).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tika import parser\n",
    "directory = r'webscraping_final'\n",
    "folder_location = r'textdata_final'\n",
    "if not os.path.exists(folder_location):\n",
    "\tos.mkdir(folder_location)\n",
    "for filename in os.listdir(directory):\n",
    "    print (filename)\n",
    "    input_file= directory + \"/\" + filename\n",
    "    file_data = parser.from_file(input_file)\n",
    "    text = file_data['content']\n",
    "    output_filepath= folder_location + \"/\" + filename + '.txt'\n",
    "    f = open(output_filepath, \"a\")\n",
    "    f.write(text)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "list_of_months =  ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "path_to_speeches = \"/Users/dhruv_batra/Desktop/SentimentAnalysis_Final/textdata_final/\"\n",
    "date_of_speeches_list = []\n",
    "for file in glob.glob(path_to_speeches + \"*.txt\"):\n",
    "    speech_file = open(file,\"r\").read()\n",
    "    speech_lines = speech_file.split(\"\\n\")\n",
    "    string = \"Delivered\" #All the dates for the speeches come after the word delivered\n",
    "    counter = 0\n",
    "    #print(speech_lines)\n",
    "    for line in speech_lines:\n",
    "        if line.startswith(string) or line.startswith(string.lower()):\n",
    "            end_index = line.find(\",\") \n",
    "            date_of_speech = line[10:end_index] #Date of speech ends with a comma\n",
    "            date_of_speech_components = date_of_speech.split(\" \")\n",
    "            if (len(date_of_speech_components) == 3 and (date_of_speech_components[1] in list_of_months)):\n",
    "                long_month_name = date_of_speech_components[1]\n",
    "                datetime_object = datetime.datetime.strptime(long_month_name, \"%B\") #converts the month name into its number\n",
    "                month_number = datetime_object.month \n",
    "                modified_date = \"%s-%s-%s\" % (date_of_speech_components[2],str(month_number),date_of_speech_components[0]) #modified date allows me to order it for time series plot\n",
    "                if(not(date_of_speech in date_of_speeches_list)): #only want one speech for every date\n",
    "                    date_of_speeches_list.append(line[10:end_index])\n",
    "                    os.rename(r'%s' % (Path(file)),r'%s.txt' % (modified_date))\n",
    "            break\n",
    "print(len(date_of_speeches_list)) #number of speeches to be used for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    " a) Text Standardization: Expanding Contractions \n",
    " \n",
    " b) Tokenization\n",
    " \n",
    " c) Lemmatization\n",
    " \n",
    " d) TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation: Cleans each speech by removing stop words & lemmatizing words to make sure they are of the same base\n",
    "# Then, vectorized each speech and used inverse document frequency to get the relative importance (weight) of each word\n",
    "# Lastly, used sklearn's train-test-split and multiple regression model to get the sentiment scores\n",
    "\n",
    "contractions = {\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I would\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"so've\": \"so have\",\n",
    "\"that's\": \"that is\",\n",
    "\"that'll\": \"that will\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"y'all\": \"you all\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\"Biden's\": \"Biden is\",\n",
    "} #Dictionary with contractions: For each speech, I removed the contractions and made it to 2 separate words\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import twitter_samples \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk import FreqDist\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "text_document_list = []\n",
    "sentiment_list = [] #polarity associated with each speech from textblob \n",
    "date_speeches = []  #Adds the date for every speech\n",
    "speeches = glob.glob(\"*.txt\")\n",
    "path_to_speeches = \"/Users/dhruv_batra/Desktop/SentimentAnalysis_Final/\"\n",
    "\n",
    "#Goes through each speech and adds the sentiment analysis score through TextBlob into the sentiment list\n",
    "#Used TextBlob as the true y values for my ML Mutliple Regression Model\n",
    "for file in glob.glob(path_to_speeches + \"*.txt\"):\n",
    "    speech = open(file,\"r\").read()\n",
    "    date_speeches.append(Path(file).stem[-10:])\n",
    "    sentiment_list.append(TextBlob(speech).sentiment.polarity*1000) #multiplied sentiment by 1000 for easier readability\n",
    "\n",
    "    for word in speech.split():\n",
    "        if word.lower() in contractions: #replaces all contractions in the dictionary as 2 words\n",
    "            speech = speech.replace(word, contractions[word.lower()])\n",
    "\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\") #Only contains alphanumeric characters (no punctuation)\n",
    "    speech_tokens = tokenizer.tokenize(speech) #Tokenization\n",
    "\n",
    "\n",
    "#Changes form of words:Lemmatizing\n",
    "    def lemmatizer(speech_tokens):\n",
    "        new_speech = []\n",
    "        for word, tag in pos_tag(speech_tokens):\n",
    "            if (tag[0:2] == \"NN\"):\n",
    "                pos = \"n\"\n",
    "            elif (tag[0:2] == \"VB\"):\n",
    "                pos = \"v\"\n",
    "            else:\n",
    "                pos = \"a\"\n",
    "            new_speech.append(WordNetLemmatizer().lemmatize(word,pos))\n",
    "        return new_speech\n",
    "\n",
    "\n",
    "#Removes unneccessary stuff/noise from Data: removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    speech_new = lemmatizer(speech_tokens)\n",
    "    speech_minus_stopwords = []\n",
    "\n",
    "    for word in speech_new:\n",
    "        if word.lower() not in stop_words:\n",
    "            speech_minus_stopwords.append(word)\n",
    "\n",
    "    filtered_speech = speech_minus_stopwords\n",
    "    filtered_speech_lower = []\n",
    "    for word in filtered_speech:\n",
    "        filtered_speech_lower.append(word.lower()) #Added all of the words that were not stop words into a list\n",
    "    \n",
    "    speech_str = \" \".join(filtered_speech_lower) #joined the words of each speech in the list to a string\n",
    "    text_document_list.append(speech_str)\n",
    "\n",
    "#Vectorized every speech in the text_document_list\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text_document_list)\n",
    "\n",
    "\n",
    "counts = vectorizer.transform(text_document_list)\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_tf = count_vect.fit_transform(text_document_list)\n",
    "\n",
    "#Inverse Document Frequency to account for the commonality of each word in the speech\n",
    "vectorize = TfidfTransformer()\n",
    "vectorize.fit(counts)\n",
    "freq = vectorize.transform(counts)\n",
    "\n",
    "X_tfidf = vectorize.fit_transform(X_tf)\n",
    "\n",
    "X_final = X_tfidf.toarray()\n",
    "print(X_final) #2D Array of all speeches where each row vector contains the vectorized form for each speech\n",
    "\n",
    "y = np.array(sentiment_list) #list of all TextBlob sentiment scores associated with each speech\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Selection\n",
    "\n",
    "a) Train-Test-Split\n",
    "\n",
    "b) Algorithm: SciKit Learn's Multiple Linear Regression\n",
    "\n",
    "c) Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% Training and 30% Testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.30) \n",
    "print(\"X_train: \")\n",
    "print(X_train)\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"X_test: \")\n",
    "print(X_test)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(\"y_train: \")\n",
    "print(y_train)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"y_test: \")\n",
    "print(y_test)\n",
    "print(y_test.shape)\n",
    "\n",
    "#Training Linear Regression (Multiple Regression)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "#print(reg.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_test) \n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_pred) #R^2 value for regression model (Variation in y that is explained by the predictors in the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Plotting Results \n",
    "\n",
    "a) Dictionary of dates and sentiment scores\n",
    "\n",
    "b) Ordering Results By Date\n",
    "\n",
    "c) Plotting Time Series Graph with Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary with keys as Dates and values as sentiment scores\n",
    "\n",
    "import sys\n",
    "print(len(date_speeches))\n",
    "list_of_sentiments = [] #list of sentiments from the machine learning model\n",
    "list_of_sentiments_final = [] \n",
    "\n",
    "for i in range(len(X_final)):\n",
    "    arr_2d = [X_final[i]] \n",
    "    prediction = reg.predict(arr_2d)\n",
    "    list_of_sentiments.append(prediction) #adds sentiment values from the ML model\n",
    "\n",
    "for sentiment in list_of_sentiments:\n",
    "    list_of_sentiments_final.append(sentiment.tolist()[0]) \n",
    "\n",
    "sentiment_dict = dict(zip(date_speeches, list_of_sentiments_final)) #dictionary w/ date and sentiment on that speech date\n",
    "print(sentiment_dict)\n",
    "    \n",
    "    \n",
    "print(len(sentiment_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordering the Dictionary by date\n",
    "del sentiment_dict[\"20105-1-9\"]\n",
    "from datetime import datetime\n",
    "\n",
    "list_sentiment_dict = []\n",
    "for key, value in sentiment_dict.items():\n",
    "    temp = {\"Date\": key, \"Sentiment\":value}\n",
    "    list_sentiment_dict.append(temp)\n",
    "\n",
    "\n",
    "list_sentiment_dict.sort(key = lambda x: datetime.strptime(x['Date'], \"%Y-%m-%d\"))\n",
    "print(list_sentiment_dict)\n",
    "print(len(list_sentiment_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the data points in a time-series graph using Bokeh\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "y_sentiment = []\n",
    "x_dates = []\n",
    "for dictionary in list_sentiment_dict:\n",
    "    y_sentiment.append(dictionary['Sentiment'])\n",
    "    x_dates.append(dictionary['Date'])\n",
    "\n",
    "dates = [datetime.strptime(x,'%Y-%m-%d').date() for x in x_dates]\n",
    "\n",
    "df = DataFrame(dates,columns=['Dates'])\n",
    "\n",
    "df.index = pd.to_datetime(df['Dates'])\n",
    "df.index.name = 'Dates'\n",
    "\n",
    "print(df.index)\n",
    "p = figure(x_axis_type = \"datetime\", plot_height = 800, plot_width = 5000, x_axis_label=\"Date of Speech\",\n",
    "           y_axis_label=\"Sentiment Score (Multiplied by 1000)\", title = \"Sentiment Analysis Score for Obama's Speeches\")\n",
    "p.line(df.index,y_sentiment, line_width = 1, color = \"red\", line_dash = \"dashed\")\n",
    "p.circle(df.index,y_sentiment,size = 5, fill_color = \"black\", color = \"black\")\n",
    "p.title.text_font_size = '30pt'\n",
    "p.xaxis.axis_label_text_font_size = \"15pt\"\n",
    "p.yaxis.axis_label_text_font_size = \"15pt\"\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
